# =============================================================================
# Baseline Models Configuration
# =============================================================================
# Configuration for traditional and deep learning baseline models

# ARIMA Configuration
arima:
  order:
    p: 5                     # Autoregressive order
    d: 1                     # Differencing order
    q: 0                     # Moving average order
  seasonal_order: null       # Set to [P, D, Q, s] for SARIMA
  # Example seasonal: [1, 1, 1, 24] for hourly data with daily seasonality

# LSTM Configuration
lstm:
  hidden_size: 128           # Hidden state dimension
  num_layers: 2              # Number of LSTM layers
  dropout: 0.1               # Dropout between layers
  bidirectional: false       # Use bidirectional LSTM

  # Variants
  variants:
    small:
      hidden_size: 64
      num_layers: 1
      dropout: 0.0

    medium:
      hidden_size: 128
      num_layers: 2
      dropout: 0.1

    large:
      hidden_size: 256
      num_layers: 3
      dropout: 0.2

# Transformer Configuration
transformer:
  d_model: 128               # Model dimension
  nhead: 8                   # Number of attention heads
  num_encoder_layers: 2      # Number of encoder layers
  dim_feedforward: 512       # FFN dimension
  dropout: 0.1               # Dropout rate
  activation: 'gelu'         # Activation function

  # Variants
  variants:
    small:
      d_model: 64
      nhead: 4
      num_encoder_layers: 1
      dim_feedforward: 256

    medium:
      d_model: 128
      nhead: 8
      num_encoder_layers: 2
      dim_feedforward: 512

    large:
      d_model: 256
      nhead: 16
      num_encoder_layers: 4
      dim_feedforward: 1024

# LTSF-Linear Family Configuration
ltsf_linear:
  # DLinear
  dlinear:
    kernel_size: 25          # Moving average kernel for decomposition
    individual: false        # Shared or individual linear per channel

  # NLinear
  nlinear:
    individual: false

  # Basic Linear
  ltsf_linear:
    dropout: 0.05

# Training Configuration for Baselines
baseline_training:
  # Shared settings
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: 'adam'
  scheduler: 'reduce_on_plateau'
  scheduler_patience: 5
  scheduler_factor: 0.5

  # Per-model overrides
  lstm:
    learning_rate: 0.001
    epochs: 100

  transformer:
    learning_rate: 0.0001
    epochs: 100
    warmup_epochs: 5

  linear:
    learning_rate: 0.001
    epochs: 50

# Comparison Settings
comparison:
  # Metrics to track
  metrics:
    - 'RMSE'
    - 'MAE'
    - 'MAPE'
    - 'R2'
    - 'SF_MAE'               # Sudden Fluctuation MAE

  # Fair comparison settings
  fair_comparison:
    same_seed: true
    same_splits: true
    same_normalization: true
    early_stopping: true
    patience: 7
