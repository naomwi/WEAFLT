# =============================================================================
# PatchTST Specific Configuration
# =============================================================================
# Reference: "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"
# Paper: https://arxiv.org/abs/2211.14730

# Base training parameters (override default)
training:
  seq_len: 96
  pred_len: 24
  batch_size: 32
  learning_rate: 0.0001      # Lower LR for transformer training
  epochs: 100                # More epochs for transformer convergence
  patience: 10

# PatchTST Architecture
patchtst:
  # Patching parameters
  patch_len: 16              # Length of each patch (paper uses 16 or 24)
  stride: 8                  # Stride between patches (typically patch_len / 2)
  padding_patch: 'end'       # Padding strategy: 'end' or 'none'

  # Transformer parameters
  d_model: 128               # Model dimension
  nhead: 8                   # Number of attention heads (d_model must be divisible by nhead)
  num_layers: 3              # Number of transformer encoder layers
  d_ff: 256                  # Feedforward network dimension

  # Regularization
  dropout: 0.1               # General dropout rate
  fc_dropout: 0.05           # FC layer dropout

  # Normalization
  use_revin: true            # Reversible Instance Normalization
  revin_affine: true         # Learnable affine parameters in RevIN

  # Position encoding
  pe: 'learnable'            # 'learnable' or 'sinusoidal'

  # Channel handling
  individual: false          # Shared vs individual heads per channel

# Different configurations for experiments
configurations:
  # Small model for quick experiments
  small:
    patch_len: 8
    stride: 4
    d_model: 64
    nhead: 4
    num_layers: 2
    d_ff: 128

  # Medium model (default)
  medium:
    patch_len: 16
    stride: 8
    d_model: 128
    nhead: 8
    num_layers: 3
    d_ff: 256

  # Large model for best performance
  large:
    patch_len: 24
    stride: 12
    d_model: 256
    nhead: 16
    num_layers: 4
    d_ff: 512

# Hyperparameter search space
hyperparameter_search:
  patch_len: [8, 16, 24, 32]
  stride_ratio: [0.25, 0.5, 0.75]  # stride = patch_len * ratio
  d_model: [64, 128, 256]
  nhead: [4, 8, 16]
  num_layers: [2, 3, 4]
  learning_rate: [0.0001, 0.0005, 0.001]

# Expected performance baselines (for reference)
expected_performance:
  # On water quality dataset (USGS)
  pred_24:
    rmse_target: 0.65
    mae_target: 0.30
  pred_96:
    rmse_target: 0.85
    mae_target: 0.40
  pred_192:
    rmse_target: 1.05
    mae_target: 0.50
